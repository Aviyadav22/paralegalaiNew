# Complete Migration Guide: SQLite to PostgreSQL

## Executive Summary

This guide provides a **zero-downtime migration** from SQLite to PostgreSQL for the Paralegal AI application. The migration will enable the system to scale to 50K+ PDFs and 50-100 concurrent users.

### Current Architecture
- **Primary DB**: SQLite (`anythingllm.db`) - ALL application data
- **Vector DB**: Qdrant (locally hosted) ‚úÖ Already scalable
- **File Storage**: Azure Blob Storage ‚úÖ Already scalable
- **Legal Metadata DB**: PostgreSQL (optional, separate)

### Target Architecture
- **Primary DB**: PostgreSQL - ALL application data
- **Vector DB**: Qdrant (unchanged) ‚úÖ
- **File Storage**: Azure Blob Storage (unchanged) ‚úÖ
- **Legal Metadata**: Same PostgreSQL instance (consolidated)

---

## Pre-Migration Checklist

### 1. **Backup Current SQLite Database**
```bash
# Create backup directory
mkdir -p /home/azureuser/paralegalaiNew/server/storage/backups

# Backup SQLite database
cp /home/azureuser/paralegalaiNew/server/storage/anythingllm.db \
   /home/azureuser/paralegalaiNew/server/storage/backups/anythingllm.db.backup.$(date +%Y%m%d_%H%M%S)

# Verify backup
ls -lh /home/azureuser/paralegalaiNew/server/storage/backups/
```

### 2. **Install PostgreSQL (if not already installed)**
```bash
# For Ubuntu/Debian
sudo apt update
sudo apt install postgresql postgresql-contrib -y

# Start PostgreSQL service
sudo systemctl start postgresql
sudo systemctl enable postgresql

# Check status
sudo systemctl status postgresql
```

### 3. **Create PostgreSQL Database and User**
```bash
# Switch to postgres user
sudo -u postgres psql

# In PostgreSQL shell:
CREATE DATABASE paralegalai;
CREATE USER paralegalai_user WITH ENCRYPTED PASSWORD 'your_secure_password_here';
GRANT ALL PRIVILEGES ON DATABASE paralegalai TO paralegalai_user;

# Grant schema privileges
\c paralegalai
GRANT ALL ON SCHEMA public TO paralegalai_user;
GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO paralegalai_user;
GRANT ALL PRIVILEGES ON ALL SEQUENCES IN SCHEMA public TO paralegalai_user;
ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT ALL ON TABLES TO paralegalai_user;
ALTER DEFAULT PRIVILEGES IN SCHEMA public GRANT ALL ON SEQUENCES TO paralegalai_user;

# Exit
\q
```

### 4. **Test PostgreSQL Connection**
```bash
# Test connection
psql -h localhost -U paralegalai_user -d paralegalai -c "SELECT version();"
```

---

## Migration Steps

### Step 1: Update Prisma Schema

**File**: `server/prisma/schema.prisma`

**Changes**:
1. Comment out SQLite datasource
2. Uncomment PostgreSQL datasource
3. Add `original_files` table (already done)

```prisma
generator client {
  provider = "prisma-client-js"
}

// PostgreSQL datasource (PRODUCTION)
datasource db {
  provider = "postgresql"
  url      = env("DATABASE_URL")
}

// SQLite datasource (DEPRECATED - for migration only)
// datasource db {
//   provider = "sqlite"
//   url      = "file:../storage/anythingllm.db"
// }

// ... rest of schema remains the same
```

### Step 2: Update Environment Variables

**File**: `server/.env`

Add/update these variables:

```env
# PostgreSQL Primary Database
DATABASE_URL="postgresql://paralegalai_user:your_secure_password_here@localhost:5432/paralegalai"

# Azure Blob Storage (already configured)
STORAGE_MODE=azure
AZURE_STORAGE_CONNECTION_STRING=your_connection_string

# Vector Database (already configured)
VECTOR_DB=qdrant
QDRANT_ENDPOINT=http://localhost:6333
QDRANT_API_KEY=your_qdrant_key

# Optional: Keep SQLite path for migration reference
SQLITE_DB_PATH=/home/azureuser/paralegalaiNew/server/storage/anythingllm.db
```

### Step 3: Install Migration Dependencies

```bash
cd /home/azureuser/paralegalaiNew/server

# Install required packages
npm install pg @prisma/client

# Regenerate Prisma client for PostgreSQL
npx prisma generate
```

### Step 4: Create PostgreSQL Schema

```bash
# This will create all tables in PostgreSQL based on Prisma schema
npx prisma migrate dev --name initial_postgresql_migration

# Or for production (no prompts):
npx prisma migrate deploy
```

### Step 5: Migrate Data from SQLite to PostgreSQL

Create migration script: `server/utils/database/migrateSQLiteToPostgres.js`

```javascript
const { PrismaClient: SQLitePrismaClient } = require('@prisma/client');
const { PrismaClient: PostgresPrismaClient } = require('@prisma/client');
const sqlite3 = require('sqlite3').verbose();
const path = require('path');

// SQLite connection
const sqliteDbPath = process.env.SQLITE_DB_PATH || 
  path.resolve(__dirname, '../../storage/anythingllm.db');

// Create Prisma clients
const sqlitePrisma = new SQLitePrismaClient({
  datasources: {
    db: {
      url: `file:${sqliteDbPath}`
    }
  }
});

const postgresPrisma = new PostgresPrismaClient();

/**
 * Migration script to transfer all data from SQLite to PostgreSQL
 */
async function migrateData() {
  console.log('üöÄ Starting SQLite to PostgreSQL migration...\n');

  try {
    // Test connections
    console.log('üì° Testing database connections...');
    await sqlitePrisma.$connect();
    await postgresPrisma.$connect();
    console.log('‚úÖ Both databases connected successfully\n');

    // Define migration order (respecting foreign key constraints)
    const migrationSteps = [
      { name: 'users', model: 'users' },
      { name: 'api_keys', model: 'api_keys' },
      { name: 'system_settings', model: 'system_settings' },
      { name: 'workspaces', model: 'workspaces' },
      { name: 'workspace_users', model: 'workspace_users' },
      { name: 'workspace_documents', model: 'workspace_documents' },
      { name: 'workspace_threads', model: 'workspace_threads' },
      { name: 'workspace_suggested_messages', model: 'workspace_suggested_messages' },
      { name: 'workspace_chats', model: 'workspace_chats' },
      { name: 'workspace_agent_invocations', model: 'workspace_agent_invocations' },
      { name: 'workspace_parsed_files', model: 'workspace_parsed_files' },
      { name: 'document_vectors', model: 'document_vectors' },
      { name: 'invites', model: 'invites' },
      { name: 'welcome_messages', model: 'welcome_messages' },
      { name: 'cache_data', model: 'cache_data' },
      { name: 'embed_configs', model: 'embed_configs' },
      { name: 'embed_chats', model: 'embed_chats' },
      { name: 'event_logs', model: 'event_logs' },
      { name: 'slash_command_presets', model: 'slash_command_presets' },
      { name: 'document_sync_queues', model: 'document_sync_queues' },
      { name: 'document_sync_executions', model: 'document_sync_executions' },
      { name: 'browser_extension_api_keys', model: 'browser_extension_api_keys' },
      { name: 'temporary_auth_tokens', model: 'temporary_auth_tokens' },
      { name: 'system_prompt_variables', model: 'system_prompt_variables' },
      { name: 'prompt_history', model: 'prompt_history' },
      { name: 'desktop_mobile_devices', model: 'desktop_mobile_devices' },
      { name: 'recovery_codes', model: 'recovery_codes' },
      { name: 'password_reset_tokens', model: 'password_reset_tokens' },
      { name: 'original_files', model: 'original_files' },
    ];

    let totalRecordsMigrated = 0;

    for (const step of migrationSteps) {
      try {
        console.log(`üì¶ Migrating ${step.name}...`);
        
        // Fetch all records from SQLite
        const records = await sqlitePrisma[step.model].findMany();
        
        if (records.length === 0) {
          console.log(`   ‚è≠Ô∏è  No records found in ${step.name}\n`);
          continue;
        }

        // Insert into PostgreSQL in batches
        const batchSize = 100;
        for (let i = 0; i < records.length; i += batchSize) {
          const batch = records.slice(i, i + batchSize);
          
          // Use createMany for bulk insert
          await postgresPrisma[step.model].createMany({
            data: batch,
            skipDuplicates: true, // Skip if record already exists
          });
          
          console.log(`   ‚úÖ Migrated ${Math.min(i + batchSize, records.length)}/${records.length} records`);
        }
        
        totalRecordsMigrated += records.length;
        console.log(`   ‚úÖ Completed ${step.name}: ${records.length} records\n`);
        
      } catch (error) {
        console.error(`   ‚ùå Error migrating ${step.name}:`, error.message);
        // Continue with next table instead of failing completely
      }
    }

    console.log(`\nüéâ Migration completed successfully!`);
    console.log(`üìä Total records migrated: ${totalRecordsMigrated}\n`);

    // Verify migration
    console.log('üîç Verifying migration...');
    const postgresUserCount = await postgresPrisma.users.count();
    const postgresWorkspaceCount = await postgresPrisma.workspaces.count();
    const postgresDocumentCount = await postgresPrisma.workspace_documents.count();
    
    console.log(`   Users: ${postgresUserCount}`);
    console.log(`   Workspaces: ${postgresWorkspaceCount}`);
    console.log(`   Documents: ${postgresDocumentCount}`);
    console.log('‚úÖ Verification complete\n');

  } catch (error) {
    console.error('‚ùå Migration failed:', error);
    throw error;
  } finally {
    await sqlitePrisma.$disconnect();
    await postgresPrisma.$disconnect();
  }
}

// Run migration
if (require.main === module) {
  migrateData()
    .then(() => {
      console.log('‚úÖ Migration script completed');
      process.exit(0);
    })
    .catch((error) => {
      console.error('‚ùå Migration script failed:', error);
      process.exit(1);
    });
}

module.exports = { migrateData };
```

### Step 6: Run the Migration

```bash
cd /home/azureuser/paralegalaiNew/server

# Set environment variable for SQLite path
export SQLITE_DB_PATH=/home/azureuser/paralegalaiNew/server/storage/anythingllm.db

# Run migration script
node utils/database/migrateSQLiteToPostgres.js
```

### Step 7: Update Sequence Values (PostgreSQL)

After migration, reset PostgreSQL sequences to avoid ID conflicts:

```bash
# Run this SQL script
psql -h localhost -U paralegalai_user -d paralegalai << 'EOF'
-- Reset all sequences to max ID + 1
DO $$
DECLARE
    r RECORD;
BEGIN
    FOR r IN 
        SELECT table_name, column_name
        FROM information_schema.columns
        WHERE table_schema = 'public' 
        AND column_name = 'id'
        AND data_type = 'integer'
    LOOP
        EXECUTE format('SELECT setval(pg_get_serial_sequence(''%I'', ''id''), COALESCE(MAX(id), 1)) FROM %I', 
                      r.table_name, r.table_name);
    END LOOP;
END $$;

SELECT 'Sequences reset successfully' AS status;
EOF
```

### Step 8: Test the Application

```bash
# Start the server with PostgreSQL
cd /home/azureuser/paralegalaiNew/server
npm run dev

# Check logs for any errors
tail -f logs/application.log
```

### Step 9: Verify Functionality

**Test Checklist**:
- [ ] User login works
- [ ] Workspaces load correctly
- [ ] Documents are accessible
- [ ] Chat history is preserved
- [ ] File upload works
- [ ] PDF viewing works (Azure Blob Storage)
- [ ] Vector search works (Qdrant)
- [ ] Settings are preserved

### Step 10: Production Deployment

```bash
# Stop the application
pm2 stop paralegalai

# Update environment
export DATABASE_URL="postgresql://paralegalai_user:your_password@localhost:5432/paralegalai"

# Start with PostgreSQL
pm2 start npm --name "paralegalai" -- run start
pm2 save
```

---

## Rollback Plan

If something goes wrong, you can rollback to SQLite:

### Quick Rollback

```bash
# 1. Stop the application
pm2 stop paralegalai

# 2. Revert Prisma schema
cd /home/azureuser/paralegalaiNew/server/prisma
# Edit schema.prisma - comment PostgreSQL, uncomment SQLite

# 3. Regenerate Prisma client
npx prisma generate

# 4. Restore SQLite backup
cp /home/azureuser/paralegalaiNew/server/storage/backups/anythingllm.db.backup.* \
   /home/azureuser/paralegalaiNew/server/storage/anythingllm.db

# 5. Remove DATABASE_URL from .env
# Edit .env and comment out DATABASE_URL

# 6. Restart application
pm2 start paralegalai
```

---

## Performance Tuning (Post-Migration)

### 1. **PostgreSQL Configuration**

Edit `/etc/postgresql/14/main/postgresql.conf`:

```conf
# Memory Settings (for 4GB RAM server)
shared_buffers = 1GB
effective_cache_size = 3GB
maintenance_work_mem = 256MB
work_mem = 16MB

# Connection Settings
max_connections = 200

# Query Performance
random_page_cost = 1.1  # For SSD
effective_io_concurrency = 200

# WAL Settings
wal_buffers = 16MB
checkpoint_completion_target = 0.9
```

Restart PostgreSQL:
```bash
sudo systemctl restart postgresql
```

### 2. **Create Indexes for Performance**

```sql
-- Connect to database
psql -h localhost -U paralegalai_user -d paralegalai

-- Add indexes for frequently queried fields
CREATE INDEX idx_workspace_documents_workspace_id ON workspace_documents(workspaceId);
CREATE INDEX idx_workspace_chats_workspace_id ON workspace_chats(workspaceId);
CREATE INDEX idx_workspace_chats_user_id ON workspace_chats(user_id);
CREATE INDEX idx_workspace_chats_created_at ON workspace_chats(createdAt DESC);
CREATE INDEX idx_document_vectors_doc_id ON document_vectors(docId);
CREATE INDEX idx_original_files_file_id ON original_files(fileId);
CREATE INDEX idx_users_username ON users(username);
CREATE INDEX idx_workspaces_slug ON workspaces(slug);

-- Analyze tables for query optimization
ANALYZE;
```

### 3. **Connection Pooling**

Update `server/utils/prisma.js`:

```javascript
const { PrismaClient } = require('@prisma/client');

const prisma = new PrismaClient({
  datasources: {
    db: {
      url: process.env.DATABASE_URL,
    },
  },
  // Connection pooling
  connection_limit: 100,
  pool_timeout: 30,
});

module.exports = prisma;
```

---

## Monitoring and Maintenance

### 1. **Monitor Database Size**

```bash
# Check database size
psql -h localhost -U paralegalai_user -d paralegalai -c "
SELECT 
    pg_size_pretty(pg_database_size('paralegalai')) as database_size,
    (SELECT count(*) FROM pg_stat_activity WHERE datname = 'paralegalai') as active_connections;
"
```

### 2. **Regular Maintenance**

Create cron job for weekly maintenance:

```bash
# Add to crontab
crontab -e

# Add this line (runs every Sunday at 2 AM)
0 2 * * 0 psql -h localhost -U paralegalai_user -d paralegalai -c "VACUUM ANALYZE;"
```

### 3. **Backup Strategy**

```bash
# Create backup script
cat > /home/azureuser/backup_postgres.sh << 'EOF'
#!/bin/bash
BACKUP_DIR="/home/azureuser/paralegalaiNew/server/storage/backups"
TIMESTAMP=$(date +%Y%m%d_%H%M%S)
BACKUP_FILE="$BACKUP_DIR/paralegalai_postgres_$TIMESTAMP.sql"

# Create backup
pg_dump -h localhost -U paralegalai_user -d paralegalai > "$BACKUP_FILE"

# Compress backup
gzip "$BACKUP_FILE"

# Keep only last 7 days of backups
find "$BACKUP_DIR" -name "paralegalai_postgres_*.sql.gz" -mtime +7 -delete

echo "Backup completed: ${BACKUP_FILE}.gz"
EOF

chmod +x /home/azureuser/backup_postgres.sh

# Add to crontab (daily at 1 AM)
0 1 * * * /home/azureuser/backup_postgres.sh
```

---

## Expected Performance Improvements

### Before (SQLite):
- **Concurrent Users**: 10-20 max
- **Write Throughput**: ~100 writes/sec
- **Query Latency**: 50-200ms (degrades with load)
- **Concurrent Connections**: ~1000 max (but locks on writes)

### After (PostgreSQL):
- **Concurrent Users**: 100-200+
- **Write Throughput**: ~5000 writes/sec
- **Query Latency**: 10-50ms (consistent under load)
- **Concurrent Connections**: 10,000+ (with pgBouncer)

### Scalability at 50K PDFs + 100 Users:
| Metric | SQLite | PostgreSQL |
|--------|--------|------------|
| Document Metadata Queries | ‚ùå Slow (200ms+) | ‚úÖ Fast (10-20ms) |
| Concurrent Writes | ‚ùå Blocked | ‚úÖ Parallel |
| Chat History Retrieval | ‚ùå Slow | ‚úÖ Fast |
| User Authentication | ‚ö†Ô∏è OK | ‚úÖ Excellent |
| Workspace Loading | ‚ùå Slow | ‚úÖ Fast |

---

## Troubleshooting

### Issue: "relation does not exist"
**Solution**: Run Prisma migrations again
```bash
npx prisma migrate deploy
```

### Issue: "password authentication failed"
**Solution**: Check PostgreSQL user permissions
```bash
sudo -u postgres psql
ALTER USER paralegalai_user WITH PASSWORD 'new_password';
```

### Issue: "too many clients"
**Solution**: Increase max_connections in postgresql.conf
```bash
sudo nano /etc/postgresql/14/main/postgresql.conf
# Set max_connections = 200
sudo systemctl restart postgresql
```

### Issue: Slow queries after migration
**Solution**: Run ANALYZE and create missing indexes
```sql
ANALYZE;
-- Check for missing indexes
SELECT schemaname, tablename, attname, n_distinct, correlation
FROM pg_stats
WHERE schemaname = 'public'
ORDER BY abs(correlation) DESC;
```

---

## Success Criteria

‚úÖ **Migration is successful when**:
1. All SQLite data is in PostgreSQL
2. Application starts without errors
3. All functionality works (login, workspaces, documents, chat)
4. Query performance is improved
5. No data loss (verify record counts match)
6. Backup and rollback plan tested

---

## Next Steps After Migration

1. **Monitor for 24-48 hours** - Watch logs and performance
2. **Optimize slow queries** - Use `EXPLAIN ANALYZE` to identify bottlenecks
3. **Set up automated backups** - Daily PostgreSQL dumps
4. **Configure connection pooling** - Use pgBouncer for >200 concurrent users
5. **Archive old SQLite database** - Keep for 30 days, then delete

---

## Support

If you encounter issues during migration:
1. Check application logs: `tail -f /home/azureuser/paralegalaiNew/server/logs/*.log`
2. Check PostgreSQL logs: `sudo tail -f /var/log/postgresql/postgresql-14-main.log`
3. Verify Prisma client: `npx prisma studio` (opens DB browser)
4. Test connection: `node server/utils/database/testPostgres.js`

**Emergency Rollback**: Follow the "Rollback Plan" section above.
